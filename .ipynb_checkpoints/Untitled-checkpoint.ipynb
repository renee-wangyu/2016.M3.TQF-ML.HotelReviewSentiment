{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import jieba\n",
    "import re\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_from_txt(fdir, stop_words):\n",
    "    \"\"\"loading data from txt \n",
    "\n",
    "    @type fdir: str\n",
    "    @type stop_words: set\n",
    "    @rtype: data List\n",
    "    @rtype: word_set Dict\n",
    "    \"\"\"\n",
    "    \n",
    "    data = []\n",
    "    word_set = set()\n",
    "    p1 = re.compile('[0-9a-zA-Z]')\n",
    "    if os.path.exists(fdir) == False:\n",
    "        print(fdir,'does not exist, try again!')\n",
    "        return None\n",
    "    for fname in os.listdir(fdir):\n",
    "        content = []\n",
    "        for line in open(fdir+'/'+fname):\n",
    "            line = p1.subn('',line)[0]\n",
    "            for word in jieba.cut(line.strip()):\n",
    "                if len(word) > 1 \\\n",
    "                   and word not in stop_words:\n",
    "                    content.append(word)\n",
    "                    word_set.add(word)\n",
    "        data.append(' '.join(content))\n",
    "    return data, word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def write_dict(word_set, fname):\n",
    "    \"\"\"write the dict into a file\n",
    "\n",
    "    @type word_set: word dictionary\n",
    "    @type fname: dictionary filename\n",
    "    \"\"\"\n",
    "    f = open(fname,'w')\n",
    "    for word in word_set:\n",
    "        #use the utf8 encode\n",
    "        f.write(word.encode('utf8')+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_seg_result(data, fname):\n",
    "    \"\"\"write the seg result into a file\n",
    "\n",
    "    @type data: seg data\n",
    "    @type fname: seg filename\n",
    "    \"\"\"\n",
    "    f = open(fname,'w')\n",
    "    for line in data:\n",
    "        #use the utf8 encode\n",
    "        f.write(line.encode('utf8')+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_result(result_data, fname):\n",
    "    \"\"\"write the result into a file\n",
    "\n",
    "    @type data: reuslt data\n",
    "    @type fname: result filename\n",
    "    \"\"\"\n",
    "    f = open(fname,'w')\n",
    "    for d in result_data:\n",
    "        s = ''\n",
    "        for dd in d[:-1]:\n",
    "            s += str(dd)+','    \n",
    "        f.write(s+str(d[-1])+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count(data, word_set):\n",
    "    \"\"\"count the data into the map\n",
    "    @type data: seg data\n",
    "    @type word_set: word dictionary\n",
    "    \"\"\"\n",
    "    wd = dict(zip(word_set, range(len(word_set))))\n",
    "    result_data = []\n",
    "    for line in data:\n",
    "        d = [0]*len(word_set)\n",
    "        for word in line.split(' '):\n",
    "            if len(word) > 0:\n",
    "                d[wd[word]] += 1\n",
    "        result_data.append(d)\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_result(fdir):\n",
    "    print ('load stop words')\n",
    "    stop_words = set([i.strip().encode('utf8').decode('utf8') for i in open('stoplis.txt')])\n",
    "    print ('load file and seg:')    \n",
    "    data,word_set = load_from_txt(fdir, stop_words)\n",
    "    print ('seg over')\n",
    "    print ('write the dict')\n",
    "    write_dict(word_set, 'my_dict_'+fdir+'.txt')\n",
    "    print ('write the seg file')\n",
    "    write_seg_result(data, 'seg_'+fdir+'.txt')\n",
    "    print ('start to word count')    \n",
    "    return data,word_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_preocess():\n",
    "    data = [i.strip().decode('utf8') for i in open('seg_pos.txt')]    \n",
    "    label_pos = np.ones((3000,1),dtype='int')\n",
    "    label_neg = np.zeros((3000,1),dtype='int')\n",
    "    label = np.vstack((label_pos,label_neg))    \n",
    "    data = data+[i.strip().decode('utf8') for i in open('seg_neg.txt')]\n",
    "    c = CountVectorizer(min_df=3,ngram_range=(1,3))\n",
    "    tfidf = TfidfTransformer()\n",
    "    tfidf_all=tfidf.fit_transform(c.fit_transform(data))\n",
    "    print(tfidf_all.shape)\n",
    "    data = tfidf_all\n",
    "    labels = label\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(data.toarray(), labels, test_size=0.4, random_state=1)\n",
    "    model = GaussianNB()\n",
    "    model.fit(xtrain, ytrain)\n",
    "    ypredict = model.predict(xtest)\n",
    "    print ('accuracy is:',accuracy_score(ytest, ypredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    pos_data = np.array([i.strip().split(',') for i in open('data_pos.txt')])\n",
    "    m1 = np.array(pos_data,dtype='int')\n",
    "    label = np.ones((3000,1),dtype='int')\n",
    "    pd = np.hstack((m1,label))\n",
    "    #print(pd.shape)\n",
    "    neg_data = np.array([i.strip().split(',') for i in open('data_neg.txt')])\n",
    "    m1 = np.array(neg_data,dtype='int')\n",
    "    label = np.zeros((3000,1),dtype='int')\n",
    "    nd = np.hstack((m1,label))\n",
    "    print (pd.shape)\n",
    "    print (nd.shape)\n",
    "    ad = np.concatenate((pd,nd))\n",
    "    data = np.array(ad[:, :-1])\n",
    "    \n",
    "    labels = np.array(ad[:,-1])\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(data, labels, test_size=0.4, random_state=1)\n",
    "    model = GaussianNB()\n",
    "    model.fit(xtrain, ytrain)\n",
    "    ypredict = model.predict(xtest)\n",
    "    print ('accuracy is:',accuracy_score(ytest, ypredict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load stop words\n",
      "load file and seg:\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xbe in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-df9a38c59254>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mposdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_set_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnegdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_set_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mword_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_set_pos\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mword_set_neg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-afcfeeb94303>\u001b[0m in \u001b[0;36mget_data_result\u001b[0;34m(fdir)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stoplis.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'load file and seg:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'seg over'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'write the dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-42-9657b759bca9>\u001b[0m in \u001b[0;36mload_from_txt\u001b[0;34m(fdir, stop_words)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjieba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/renee/anaconda/lib/python3.5/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xbe in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    # main function    \n",
    "    posdata,word_set_pos = get_data_result('pos')\n",
    "    negdata,word_set_neg = get_data_result('neg')\n",
    "    word_set = word_set_pos | word_set_neg\n",
    "    result_data = word_count(posdata, word_set)\n",
    "    write_result(result_data, 'data_pos.txt')\n",
    "    result_data = word_count(negdata, word_set)\n",
    "    write_result(result_data, 'data_neg.txt')\n",
    "    text_preocess()\n",
    "    #predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
